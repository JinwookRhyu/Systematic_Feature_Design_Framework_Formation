{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn==1.3.2 in c:\\users\\jinwook\\anaconda3\\envs\\hdreganalytics\\lib\\site-packages (1.3.2)\n",
      "Requirement already satisfied: scipy>=1.5.0 in c:\\users\\jinwook\\anaconda3\\envs\\hdreganalytics\\lib\\site-packages (from scikit-learn==1.3.2) (1.12.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\jinwook\\anaconda3\\envs\\hdreganalytics\\lib\\site-packages (from scikit-learn==1.3.2) (3.3.0)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\jinwook\\anaconda3\\envs\\hdreganalytics\\lib\\site-packages (from scikit-learn==1.3.2) (1.3.2)\n",
      "Requirement already satisfied: numpy<2.0,>=1.17.3 in c:\\users\\jinwook\\anaconda3\\envs\\hdreganalytics\\lib\\site-packages (from scikit-learn==1.3.2) (1.26.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install scikit-learn==1.3.2 \n",
    "# This is the version for zipped SPA_results. Please change it to appropriate version if you are collecting pickle files by yourself using the run_SPA code\n",
    "\n",
    "mode = 'agnostic' # 'agnostic' / 'autoML' / 'designed'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log_protocol_params\n",
      "log_protocol_params_reduced\n",
      "protocol_params\n",
      "protocol_params_reduced\n"
     ]
    }
   ],
   "source": [
    "from matplotlib.pyplot import cm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "import glob, os\n",
    "\n",
    "if not os.path.exists(os.path.dirname(os.getcwd()) + \"/summarized_results\"):\n",
    "    os.mkdir(os.path.dirname(os.getcwd()) + \"/summarized_results\")\n",
    "\n",
    "if mode == 'agnostic':\n",
    "\n",
    "    dir_pklfile = os.path.dirname(os.getcwd()) + \"/SPA_results_agnostic\"\n",
    "    data_name_list = os.listdir(dir_pklfile)\n",
    "\n",
    "    model_name_list_all = ['SVR', 'RF', 'EN', 'RR', 'PLS', 'SPLS', 'ALVEN1', 'LCEN1', 'ALVEN2', 'LCEN2', 'ALVEN3', 'LCEN3', 'XGB']\n",
    "\n",
    "    df_data = [[0]*39 for i in range(len(data_name_list) * len(model_name_list_all))]\n",
    "\n",
    "    ind_model = 0\n",
    "\n",
    "    for ii in range(len(data_name_list)):\n",
    "\n",
    "        data_name = data_name_list[ii].replace('.pkl', '')\n",
    "        print(data_name)\n",
    "\n",
    "        with open(dir_pklfile + \"/\" + data_name + \".pkl\", 'rb') as file:\n",
    "            history = pickle.load(file)\n",
    "\n",
    "        model_name_list = [i for i in history]\n",
    "\n",
    "        for model_name in model_name_list:\n",
    "            num_nest = len(history[model_name])\n",
    "\n",
    "            y_train_list = []\n",
    "            yhat_train_list = []\n",
    "            y_test_list = []\n",
    "            yhat_test_list = []\n",
    "            train_nest_rmse = np.full((5,), np.nan)\n",
    "            test_nest_rmse = np.full((5,), np.nan)\n",
    "            train_nest_mape = np.full((5,), np.nan)\n",
    "            test_nest_mape = np.full((5,), np.nan)\n",
    "\n",
    "            if num_nest > 0:\n",
    "\n",
    "                for i in list(history[model_name].keys()):\n",
    "                    if i in history[model_name]:\n",
    "                        y_train = history[model_name][i]['y_train']\n",
    "                        yhat_train = history[model_name][i]['yhat_train']\n",
    "                        y_test = history[model_name][i]['y_test']\n",
    "                        yhat_test = history[model_name][i]['yhat_test']\n",
    "\n",
    "                        if \"log\" in data_name:\n",
    "                            y_train = np.exp(y_train)\n",
    "                            yhat_train = np.exp(yhat_train)\n",
    "                            y_test = np.exp(y_test)\n",
    "                            yhat_test = np.exp(np.clip(yhat_test, -np.inf, 300))\n",
    "\n",
    "                        y_train_list = np.append(y_train_list, y_train.flatten())\n",
    "                        yhat_train_list = np.append(yhat_train_list, yhat_train.flatten())\n",
    "                        y_test_list = np.append(y_test_list, y_test.flatten())\n",
    "                        yhat_test_list = np.append(yhat_test_list, yhat_test.flatten())\n",
    "\n",
    "                    train_nest_rmse[i] = np.sqrt(np.sum((yhat_train - y_train) ** 2) / y_train.shape[0])\n",
    "                    test_nest_rmse[i] = np.sqrt(np.sum((yhat_test - y_test) ** 2) / y_test.shape[0])\n",
    "                    train_nest_mape[i] = np.sum(np.divide(np.abs(yhat_train - y_train), y_train) * 100) / y_train.shape[0]\n",
    "                    test_nest_mape[i] = np.sum(np.divide(np.abs(yhat_test - y_test), y_test) * 100) / y_test.shape[0]\n",
    "\n",
    "                if 'reduced' in data_name:\n",
    "                    df_data[ind_model][0] = True\n",
    "                    if 'log' in data_name:\n",
    "                        df_data[ind_model][1] = True\n",
    "                        items = data_name.split('_')\n",
    "                    else:\n",
    "                        df_data[ind_model][1] = False\n",
    "                        items = data_name.split('_')\n",
    "                else:\n",
    "                    df_data[ind_model][0] = False\n",
    "                    if 'log' in data_name:\n",
    "                        df_data[ind_model][1] = True\n",
    "                        items = data_name.split('_')\n",
    "                    else:\n",
    "                        df_data[ind_model][1] = False\n",
    "                        items = data_name.split('_')\n",
    "\n",
    "                train_nest_rmse_pair = np.full((int(0.5 * num_nest * (num_nest + 1)),), np.nan)\n",
    "                test_nest_rmse_pair = np.full((int(0.5 * num_nest * (num_nest + 1)),), np.nan)\n",
    "                train_nest_mape_pair = np.full((int(0.5 * num_nest * (num_nest + 1)),), np.nan)\n",
    "                test_nest_mape_pair = np.full((int(0.5 * num_nest * (num_nest + 1)),), np.nan)\n",
    "                k_pair = 0\n",
    "                for iii in range(num_nest):\n",
    "                    for jjj in range(iii + 1):\n",
    "                        train_nest_rmse_pair[k_pair] = 0.5 * (train_nest_rmse[iii] + train_nest_rmse[jjj])\n",
    "                        test_nest_rmse_pair[k_pair] = 0.5 * (test_nest_rmse[iii] + test_nest_rmse[jjj])\n",
    "                        train_nest_mape_pair[k_pair] = 0.5 * (train_nest_mape[iii] + train_nest_mape[jjj])\n",
    "                        test_nest_mape_pair[k_pair] = 0.5 * (test_nest_mape[iii] + test_nest_mape[jjj])\n",
    "                        k_pair = k_pair + 1\n",
    "\n",
    "                df_data[ind_model][2] = model_name\n",
    "                df_data[ind_model][3:8] = train_nest_rmse\n",
    "                df_data[ind_model][8:13] = test_nest_rmse\n",
    "                df_data[ind_model][13:18] = train_nest_mape\n",
    "                df_data[ind_model][18:23] = test_nest_mape\n",
    "                df_data[ind_model][23] = np.nanmean(train_nest_rmse)\n",
    "                df_data[ind_model][24] = np.nanmean(test_nest_rmse)\n",
    "                df_data[ind_model][25] = np.nanmean(train_nest_mape)\n",
    "                df_data[ind_model][26] = np.nanmean(test_nest_mape)\n",
    "                df_data[ind_model][27] = np.nanmedian(train_nest_rmse)\n",
    "                df_data[ind_model][28] = np.nanmedian(test_nest_rmse)\n",
    "                df_data[ind_model][29] = np.nanmedian(train_nest_mape)\n",
    "                df_data[ind_model][30] = np.nanmedian(test_nest_mape)\n",
    "                df_data[ind_model][31] = np.nanmax(train_nest_rmse)\n",
    "                df_data[ind_model][32] = np.nanmax(test_nest_rmse)\n",
    "                df_data[ind_model][33] = np.nanmax(train_nest_mape)\n",
    "                df_data[ind_model][34] = np.nanmax(test_nest_mape)\n",
    "                df_data[ind_model][35] = np.nanmedian(train_nest_rmse_pair)\n",
    "                df_data[ind_model][36] = np.nanmedian(test_nest_rmse_pair)\n",
    "                df_data[ind_model][37] = np.nanmedian(train_nest_mape_pair)\n",
    "                df_data[ind_model][38] = np.nanmedian(test_nest_mape_pair)\n",
    "                ind_model += 1\n",
    "\n",
    "    df = pd.DataFrame(df_data, columns=['if_reduced', 'if_log', 'model', 'train_rmse_1', 'train_rmse_2', 'train_rmse_3', 'train_rmse_4', 'train_rmse_5', 'test_rmse_1', 'test_rmse_2', 'test_rmse_3', 'test_rmse_4', 'test_rmse_5', 'train_mape_1', 'train_mape_2', 'train_mape_3', 'train_mape_4', 'train_mape_5', 'test_mape_1', 'test_mape_2', 'test_mape_3', 'test_mape_4', 'test_mape_5', 'mean_train_rmse', 'mean_test_rmse', 'mean_train_mape', 'mean_test_mape', 'med_train_rmse', 'med_test_rmse', 'med_train_mape', 'med_test_mape', 'max_train_rmse', 'max_test_rmse', 'max_train_mape', 'max_test_mape', 'HL_train_rmse', 'HL_test_rmse', 'HL_train_mape', 'HL_test_mape'])\n",
    "    df.to_csv(os.path.dirname(os.getcwd()) + \"/summarized_results/agnostic_results.csv\")\n",
    "\n",
    "elif mode == 'autoML':\n",
    "\n",
    "    dir_pklfile = os.path.dirname(os.getcwd()) + \"/SPA_results_autoML\"\n",
    "    data_name_list = os.listdir(dir_pklfile)\n",
    "\n",
    "    model_name_list_all = ['SVR', 'RF', 'EN', 'ALVEN', 'LCEN', 'XGB']\n",
    "\n",
    "    df_data = [[0] * 42 for i in range(len(data_name_list) * len(model_name_list_all))]\n",
    "\n",
    "    ind_model = 0\n",
    "\n",
    "    for ii in range(len(data_name_list)):\n",
    "\n",
    "        data_name = data_name_list[ii].replace('.pkl', '')\n",
    "        print(data_name)\n",
    "\n",
    "        with open(dir_pklfile + \"/\" + data_name + \".pkl\", 'rb') as file:\n",
    "            history = pickle.load(file)\n",
    "\n",
    "        model_name_list = [i for i in history]\n",
    "\n",
    "        for model_name in model_name_list:\n",
    "\n",
    "            y_train_list = []\n",
    "            yhat_train_list = []\n",
    "            y_test_list = []\n",
    "            yhat_test_list = []\n",
    "            train_nest_rmse = []\n",
    "            test_nest_rmse = []\n",
    "            train_nest_mape = []\n",
    "            test_nest_mape = []\n",
    "\n",
    "            num_nest = len(history[model_name])\n",
    "            if num_nest > 0:\n",
    "\n",
    "                for i in list(history[model_name].keys()):\n",
    "\n",
    "                    y_train = history[model_name][i]['y_train']\n",
    "                    yhat_train = history[model_name][i]['yhat_train']\n",
    "                    y_test = history[model_name][i]['y_test']\n",
    "                    yhat_test = history[model_name][i]['yhat_test']\n",
    "\n",
    "                    if \"log\" in data_name:\n",
    "                        y_train = np.exp(y_train)\n",
    "                        yhat_train = np.exp(yhat_train)\n",
    "                        y_test = np.exp(y_test)\n",
    "                        yhat_test = np.exp(yhat_test)\n",
    "\n",
    "                    y_train_list = np.append(y_train_list, y_train.flatten())\n",
    "                    yhat_train_list = np.append(yhat_train_list, yhat_train.flatten())\n",
    "                    y_test_list = np.append(y_test_list, y_test.flatten())\n",
    "                    yhat_test_list = np.append(yhat_test_list, yhat_test.flatten())\n",
    "\n",
    "                    train_nest_rmse = np.append(train_nest_rmse,\n",
    "                                                np.sqrt(np.sum((yhat_train - y_train) ** 2) / y_train.shape[0]))\n",
    "                    test_nest_rmse = np.append(test_nest_rmse,\n",
    "                                               np.sqrt(np.sum((yhat_test - y_test) ** 2) / y_test.shape[0]))\n",
    "                    train_nest_mape = np.append(train_nest_mape,\n",
    "                                                np.sum(np.divide(np.abs(yhat_train - y_train), y_train) * 100) /\n",
    "                                                y_train.shape[\n",
    "                                                    0])\n",
    "                    test_nest_mape = np.append(test_nest_mape,\n",
    "                                               np.sum(np.divide(np.abs(yhat_test - y_test), y_test) * 100) /\n",
    "                                               y_test.shape[0])\n",
    "\n",
    "                if 'tsfresh' in data_name:\n",
    "                    df_data[ind_model][3] = True\n",
    "                    if 'log' in data_name:\n",
    "                        df_data[ind_model][4] = True\n",
    "                        items = data_name.split('_')\n",
    "                        df_data[ind_model][0] = items[1]\n",
    "                        df_data[ind_model][1] = items[2] + \"_\" + items[3]\n",
    "                        df_data[ind_model][2] = float(items[7])\n",
    "                    else:\n",
    "                        df_data[ind_model][4] = False\n",
    "                        items = data_name.split('_')\n",
    "                        df_data[ind_model][0] = items[0]\n",
    "                        df_data[ind_model][1] = items[1] + \"_\" + items[2]\n",
    "                        df_data[ind_model][2] = float(items[6])\n",
    "                else:\n",
    "                    df_data[ind_model][3] = False\n",
    "                    if 'log' in data_name:\n",
    "                        df_data[ind_model][4] = True\n",
    "                        items = data_name.split('_')\n",
    "                        df_data[ind_model][0] = items[1]\n",
    "                        df_data[ind_model][1] = items[2] + \"_\" + items[3]\n",
    "                        df_data[ind_model][2] = float(items[6])\n",
    "                    else:\n",
    "                        df_data[ind_model][4] = False\n",
    "                        items = data_name.split('_')\n",
    "                        df_data[ind_model][0] = items[0]\n",
    "                        df_data[ind_model][1] = items[1] + \"_\" + items[2]\n",
    "                        df_data[ind_model][2] = float(items[5])\n",
    "\n",
    "                train_nest_rmse_pair = np.full((int(0.5 * num_nest * (num_nest + 1)),), np.nan)\n",
    "                test_nest_rmse_pair = np.full((int(0.5 * num_nest * (num_nest + 1)),), np.nan)\n",
    "                train_nest_mape_pair = np.full((int(0.5 * num_nest * (num_nest + 1)),), np.nan)\n",
    "                test_nest_mape_pair = np.full((int(0.5 * num_nest * (num_nest + 1)),), np.nan)\n",
    "                k_pair = 0\n",
    "                for iii in range(num_nest):\n",
    "                    for jjj in range(iii + 1):\n",
    "                        train_nest_rmse_pair[k_pair] = 0.5 * (train_nest_rmse[iii] + train_nest_rmse[jjj])\n",
    "                        test_nest_rmse_pair[k_pair] = 0.5 * (test_nest_rmse[iii] + test_nest_rmse[jjj])\n",
    "                        train_nest_mape_pair[k_pair] = 0.5 * (train_nest_mape[iii] + train_nest_mape[jjj])\n",
    "                        test_nest_mape_pair[k_pair] = 0.5 * (test_nest_mape[iii] + test_nest_mape[jjj])\n",
    "                        k_pair = k_pair + 1\n",
    "\n",
    "                df_data[ind_model][5] = model_name\n",
    "                df_data[ind_model][6:11] = train_nest_rmse\n",
    "                df_data[ind_model][11:16] = test_nest_rmse\n",
    "                df_data[ind_model][16:21] = train_nest_mape\n",
    "                df_data[ind_model][21:26] = test_nest_mape\n",
    "                df_data[ind_model][26] = np.mean(train_nest_rmse)\n",
    "                df_data[ind_model][27] = np.mean(test_nest_rmse)\n",
    "                df_data[ind_model][28] = np.mean(train_nest_mape)\n",
    "                df_data[ind_model][29] = np.mean(test_nest_mape)\n",
    "                df_data[ind_model][30] = np.median(train_nest_rmse)\n",
    "                df_data[ind_model][31] = np.median(test_nest_rmse)\n",
    "                df_data[ind_model][32] = np.median(train_nest_mape)\n",
    "                df_data[ind_model][33] = np.median(test_nest_mape)\n",
    "                df_data[ind_model][34] = np.max(train_nest_rmse)\n",
    "                df_data[ind_model][35] = np.max(test_nest_rmse)\n",
    "                df_data[ind_model][36] = np.max(train_nest_mape)\n",
    "                df_data[ind_model][37] = np.max(test_nest_mape)\n",
    "                df_data[ind_model][38] = np.median(train_nest_rmse_pair)\n",
    "                df_data[ind_model][39] = np.median(test_nest_rmse_pair)\n",
    "                df_data[ind_model][40] = np.median(train_nest_mape_pair)\n",
    "                df_data[ind_model][41] = np.median(test_nest_mape_pair)\n",
    "                ind_model += 1\n",
    "\n",
    "    df = pd.DataFrame(df_data,\n",
    "                      columns=['Region', 'State_var', 'Significance', 'if_tsfresh', 'if_log', 'model', 'train_rmse_1',\n",
    "                               'train_rmse_2', 'train_rmse_3', 'train_rmse_4', 'train_rmse_5', 'test_rmse_1',\n",
    "                               'test_rmse_2', 'test_rmse_3', 'test_rmse_4', 'test_rmse_5', 'train_mape_1',\n",
    "                               'train_mape_2', 'train_mape_3', 'train_mape_4', 'train_mape_5', 'test_mape_1',\n",
    "                               'test_mape_2', 'test_mape_3', 'test_mape_4', 'test_mape_5', 'mean_train_rmse',\n",
    "                               'mean_test_rmse', 'mean_train_mape', 'mean_test_mape', 'med_train_rmse', 'med_test_rmse',\n",
    "                               'med_train_mape', 'med_test_mape', 'max_train_rmse', 'max_test_rmse', 'max_train_mape',\n",
    "                               'max_test_mape', 'HL_train_rmse', 'HL_test_rmse', 'HL_train_mape', 'HL_test_mape'])\n",
    "    df.to_csv(os.path.dirname(os.getcwd()) + '/summarized_results/autoML_results.csv')\n",
    "\n",
    "elif mode == 'designed':\n",
    "    dir_pklfile = os.path.dirname(os.getcwd()) + \"/SPA_results_designed\"\n",
    "    data_name_list = [\"log_B_Q_V_designed.pkl\"]\n",
    "\n",
    "    model_name_list_all = ['SVR', 'RF', 'EN', 'RR', 'PLS', 'SPLS', 'LCEN1', 'ALVEN1', 'LCEN2', 'ALVEN2', 'LCEN3',\n",
    "                           'ALVEN3', 'XGB']\n",
    "\n",
    "    df_data = [[0] * 38 for i in range(len(data_name_list) * len(model_name_list_all))]\n",
    "\n",
    "    ind_model = 0\n",
    "\n",
    "    for ii in range(len(data_name_list)):\n",
    "\n",
    "        data_name = data_name_list[ii].replace('.pkl', '')\n",
    "\n",
    "        with open(dir_pklfile + \"/\" + data_name + \".pkl\", 'rb') as file:\n",
    "            history = pickle.load(file)\n",
    "\n",
    "        model_name_list = [i for i in history]\n",
    "\n",
    "        for model_name in model_name_list:\n",
    "\n",
    "            num_nest = len(history[model_name])\n",
    "\n",
    "            y_train_list = []\n",
    "            yhat_train_list = []\n",
    "            y_test_list = []\n",
    "            yhat_test_list = []\n",
    "            train_nest_rmse = np.full((5,), np.nan)\n",
    "            test_nest_rmse = np.full((5,), np.nan)\n",
    "            train_nest_mape = np.full((5,), np.nan)\n",
    "            test_nest_mape = np.full((5,), np.nan)\n",
    "\n",
    "            for i in range(num_nest):\n",
    "                if i in history[model_name]:\n",
    "                    y_train = history[model_name][i]['y_train']\n",
    "                    yhat_train = history[model_name][i]['yhat_train']\n",
    "                    y_test = history[model_name][i]['y_test']\n",
    "                    yhat_test = history[model_name][i]['yhat_test']\n",
    "\n",
    "                    if \"log\" in data_name:\n",
    "                        y_train = np.exp(y_train)\n",
    "                        yhat_train = np.exp(yhat_train)\n",
    "                        y_test = np.exp(y_test)\n",
    "                        yhat_test = np.exp(np.clip(yhat_test, -np.inf, 300))\n",
    "\n",
    "                    y_train_list = np.append(y_train_list, y_train.flatten())\n",
    "                    yhat_train_list = np.append(yhat_train_list, yhat_train.flatten())\n",
    "                    y_test_list = np.append(y_test_list, y_test.flatten())\n",
    "                    yhat_test_list = np.append(yhat_test_list, yhat_test.flatten())\n",
    "\n",
    "                train_nest_rmse[i] = np.sqrt(np.sum((yhat_train - y_train) ** 2) / y_train.shape[0])\n",
    "                test_nest_rmse[i] = np.sqrt(np.sum((yhat_test - y_test) ** 2) / y_test.shape[0])\n",
    "                train_nest_mape[i] = np.sum(np.divide(np.abs(yhat_train - y_train), y_train) * 100) / y_train.shape[0]\n",
    "                test_nest_mape[i] = np.sum(np.divide(np.abs(yhat_test - y_test), y_test) * 100) / y_test.shape[0]\n",
    "\n",
    "            # ax.legend(loc='lower right', fontsize = 10)\n",
    "            train_nest_rmse_pair = np.full((int(0.5 * num_nest * (num_nest + 1)),), np.nan)\n",
    "            test_nest_rmse_pair = np.full((int(0.5 * num_nest * (num_nest + 1)),), np.nan)\n",
    "            train_nest_mape_pair = np.full((int(0.5 * num_nest * (num_nest + 1)),), np.nan)\n",
    "            test_nest_mape_pair = np.full((int(0.5 * num_nest * (num_nest + 1)),), np.nan)\n",
    "            k_pair = 0\n",
    "            for iii in range(num_nest):\n",
    "                for jjj in range(iii + 1):\n",
    "                    train_nest_rmse_pair[k_pair] = 0.5 * (train_nest_rmse[iii] + train_nest_rmse[jjj])\n",
    "                    test_nest_rmse_pair[k_pair] = 0.5 * (test_nest_rmse[iii] + test_nest_rmse[jjj])\n",
    "                    train_nest_mape_pair[k_pair] = 0.5 * (train_nest_mape[iii] + train_nest_mape[jjj])\n",
    "                    test_nest_mape_pair[k_pair] = 0.5 * (test_nest_mape[iii] + test_nest_mape[jjj])\n",
    "                    k_pair = k_pair + 1\n",
    "\n",
    "            df_data[ind_model][0] = data_name\n",
    "            df_data[ind_model][1] = model_name\n",
    "            df_data[ind_model][2:7] = train_nest_rmse\n",
    "            df_data[ind_model][7:12] = test_nest_rmse\n",
    "            df_data[ind_model][12:17] = train_nest_mape\n",
    "            df_data[ind_model][17:22] = test_nest_mape\n",
    "            df_data[ind_model][22] = np.nanmean(train_nest_rmse)\n",
    "            df_data[ind_model][23] = np.nanmean(test_nest_rmse)\n",
    "            df_data[ind_model][24] = np.nanmean(train_nest_mape)\n",
    "            df_data[ind_model][25] = np.nanmean(test_nest_mape)\n",
    "            df_data[ind_model][26] = np.nanmedian(train_nest_rmse)\n",
    "            df_data[ind_model][27] = np.nanmedian(test_nest_rmse)\n",
    "            df_data[ind_model][28] = np.nanmedian(train_nest_mape)\n",
    "            df_data[ind_model][29] = np.nanmedian(test_nest_mape)\n",
    "            df_data[ind_model][30] = np.nanmax(train_nest_rmse)\n",
    "            df_data[ind_model][31] = np.nanmax(test_nest_rmse)\n",
    "            df_data[ind_model][32] = np.nanmax(train_nest_mape)\n",
    "            df_data[ind_model][33] = np.nanmax(test_nest_mape)\n",
    "            df_data[ind_model][34] = np.nanmedian(train_nest_rmse_pair)\n",
    "            df_data[ind_model][35] = np.nanmedian(test_nest_rmse_pair)\n",
    "            df_data[ind_model][36] = np.nanmedian(train_nest_mape_pair)\n",
    "            df_data[ind_model][37] = np.nanmedian(test_nest_mape_pair)\n",
    "            ind_model += 1\n",
    "\n",
    "    df = pd.DataFrame(df_data, columns=['data', 'model', 'train_rmse_1', 'train_rmse_2', 'train_rmse_3', 'train_rmse_4',\n",
    "                                        'train_rmse_5', 'test_rmse_1', 'test_rmse_2', 'test_rmse_3', 'test_rmse_4',\n",
    "                                        'test_rmse_5', 'train_mape_1', 'train_mape_2', 'train_mape_3', 'train_mape_4',\n",
    "                                        'train_mape_5', 'test_mape_1', 'test_mape_2', 'test_mape_3', 'test_mape_4',\n",
    "                                        'test_mape_5', 'mean_train_rmse', 'mean_test_rmse', 'mean_train_mape',\n",
    "                                        'mean_test_mape', 'med_train_rmse', 'med_test_rmse', 'med_train_mape',\n",
    "                                        'med_test_mape', 'max_train_rmse', 'max_test_rmse', 'max_train_mape',\n",
    "                                        'max_test_mape', 'HL_train_rmse', 'HL_test_rmse', 'HL_train_mape',\n",
    "                                        'HL_test_mape'])\n",
    "    df.to_csv(os.path.dirname(os.getcwd()) + '/summarized_results/designed_results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HDRegAnalytics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
